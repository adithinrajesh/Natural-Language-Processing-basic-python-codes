{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e939c0e5-6863-432a-84d7-74dae5d31eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "029c2c37-7617-4da6-877d-2d879364c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Britain’s communications intelligence agency GCHQ has issued a statement denying it wiretapped Donald Trump during the US presidential campaign. The unusual move by the agency came after White House Press Secretary Sean Spicer cited claims first made on US TV channel Fox News earlier this week. GCHQ responded that the allegations were “nonsense, utterly ridiculous and should be ignored”. Former judge Andrew Napolitano initially made the claims of GCHQ involvement. Mr Spicer quoted Mr Napolitano: “Three intelligence sources have informed Fox News that President Obama went outside the chain of command.”\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea41d667-686e-49fb-b52f-524a11b1cff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Britain’s communications intelligence agency GCHQ has issued a statement denying it wiretapped Donald Trump during the US presidential campaign.\n",
      "The unusual move by the agency came after White House Press Secretary Sean Spicer cited claims first made on US TV channel Fox News earlier this week.\n",
      "GCHQ responded that the allegations were “nonsense, utterly ridiculous and should be ignored”.\n",
      "Former judge Andrew Napolitano initially made the claims of GCHQ involvement.\n",
      "Mr Spicer quoted Mr Napolitano: “Three intelligence sources have informed Fox News that President Obama went outside the chain of command.”\n",
      "\n",
      "['Britain', '’', 's', 'communications', 'intelligence', 'agency', 'GCHQ', 'has', 'issued', 'a', 'statement', 'denying', 'it', 'wiretapped', 'Donald', 'Trump', 'during', 'the', 'US', 'presidential', 'campaign', '.', 'The', 'unusual', 'move', 'by', 'the', 'agency', 'came', 'after', 'White', 'House', 'Press', 'Secretary', 'Sean', 'Spicer', 'cited', 'claims', 'first', 'made', 'on', 'US', 'TV', 'channel', 'Fox', 'News', 'earlier', 'this', 'week', '.', 'GCHQ', 'responded', 'that', 'the', 'allegations', 'were', '“', 'nonsense', ',', 'utterly', 'ridiculous', 'and', 'should', 'be', 'ignored', '”', '.', 'Former', 'judge', 'Andrew', 'Napolitano', 'initially', 'made', 'the', 'claims', 'of', 'GCHQ', 'involvement', '.', 'Mr', 'Spicer', 'quoted', 'Mr', 'Napolitano', ':', '“', 'Three', 'intelligence', 'sources', 'have', 'informed', 'Fox', 'News', 'that', 'President', 'Obama', 'went', 'outside', 'the', 'chain', 'of', 'command', '.', '”']\n"
     ]
    }
   ],
   "source": [
    "# 1- tokenization\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "\n",
    "#break it down into sentences\n",
    "sent_tokenizer = PunktSentenceTokenizer()\n",
    "sents = sent_tokenizer.tokenize(text)\n",
    "for sent in sents:\n",
    "    print(sent)\n",
    "    \n",
    "print()\n",
    "\n",
    "#break it down into tokens\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a784532-156a-4a8c-8ea7-3d784eee1368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Britain', '’', 'communications', 'intelligence', 'agency', 'GCHQ', 'issued', 'statement', 'denying', 'wiretapped', 'Donald', 'Trump', 'US', 'presidential', 'campaign', '.', 'unusual', 'move', 'agency', 'came', 'White', 'House', 'Press', 'Secretary', 'Sean', 'Spicer', 'cited', 'claims', 'first', 'made', 'US', 'TV', 'channel', 'Fox', 'News', 'earlier', 'week', '.', 'GCHQ', 'responded', 'allegations', '“', 'nonsense', ',', 'utterly', 'ridiculous', 'ignored', '”', '.', 'Former', 'judge', 'Andrew', 'Napolitano', 'initially', 'made', 'claims', 'GCHQ', 'involvement', '.', 'Mr', 'Spicer', 'quoted', 'Mr', 'Napolitano', ':', '“', 'Three', 'intelligence', 'sources', 'informed', 'Fox', 'News', 'President', 'Obama', 'went', 'outside', 'chain', 'command', '.', '”']\n"
     ]
    }
   ],
   "source": [
    "#Removal of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91f6fc9d-a87c-4432-a251-b6c10a1dfa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Synset =  car.n.01\n",
      "\n",
      "Definition =  a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "\n",
      "Hyponyms =  [Synset('ambulance.n.01'), Synset('beach_wagon.n.01'), Synset('bus.n.04'), Synset('cab.n.03'), Synset('compact.n.03'), Synset('convertible.n.01'), Synset('coupe.n.01'), Synset('cruiser.n.01'), Synset('electric.n.01'), Synset('gas_guzzler.n.01'), Synset('hardtop.n.01'), Synset('hatchback.n.01'), Synset('horseless_carriage.n.01'), Synset('hot_rod.n.01'), Synset('jeep.n.01'), Synset('limousine.n.01'), Synset('loaner.n.02'), Synset('minicar.n.01'), Synset('minivan.n.01'), Synset('model_t.n.01'), Synset('pace_car.n.01'), Synset('racer.n.02'), Synset('roadster.n.01'), Synset('sedan.n.01'), Synset('sport_utility.n.01'), Synset('sports_car.n.01'), Synset('stanley_steamer.n.01'), Synset('stock_car.n.01'), Synset('subcompact.n.01'), Synset('touring_car.n.01'), Synset('used-car.n.01')]\n",
      "\n",
      "Hypernyms =  [Synset('motor_vehicle.n.01')]\n",
      "\n",
      "Synset =  car.n.02\n",
      "\n",
      "Definition =  a wheeled vehicle adapted to the rails of railroad\n",
      "\n",
      "Hyponyms =  [Synset('baggage_car.n.01'), Synset('cabin_car.n.01'), Synset('club_car.n.01'), Synset('freight_car.n.01'), Synset('guard's_van.n.01'), Synset('handcar.n.01'), Synset('mail_car.n.01'), Synset('passenger_car.n.01'), Synset('slip_coach.n.01'), Synset('tender.n.04'), Synset('van.n.03')]\n",
      "\n",
      "Hypernyms =  [Synset('wheeled_vehicle.n.01')]\n",
      "\n",
      "Synset =  car.n.03\n",
      "\n",
      "Definition =  the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
      "\n",
      "Hypernyms =  [Synset('compartment.n.02')]\n",
      "\n",
      "Synset =  car.n.04\n",
      "\n",
      "Definition =  where passengers ride up and down\n",
      "\n",
      "Hypernyms =  [Synset('compartment.n.02')]\n",
      "\n",
      "Synset =  cable_car.n.01\n",
      "\n",
      "Definition =  a conveyance for passengers or freight on a cable railway\n",
      "\n",
      "Hypernyms =  [Synset('compartment.n.02')]\n"
     ]
    }
   ],
   "source": [
    "#synsets, hyponyms, hypernyms\n",
    "from nltk.corpus import wordnet\n",
    "synsets = wordnet.synsets('car')\n",
    "for synset in synsets:\n",
    "    print(\"\\nSynset = \", synset.name())\n",
    "    print(\"\\nDefinition = \",synset.definition())\n",
    "    \n",
    "    hyponyms = synset.hyponyms()\n",
    "    if hyponyms:\n",
    "        print(\"\\nHyponyms = \",hyponyms)\n",
    "    \n",
    "    hypernyms = synset.hypernyms()\n",
    "    if hypernyms:\n",
    "        print(\"\\nHypernyms = \",hypernyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cf7d977-d27b-48d6-9afe-25f1b046617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['Britain', '’', 's', 'communications', 'intelligence', 'agency', 'GCHQ', 'has', 'issued', 'a', 'statement', 'denying', 'it', 'wiretapped', 'Donald', 'Trump', 'during', 'the', 'US', 'presidential', 'campaign', '.', 'The', 'unusual', 'move', 'by', 'the', 'agency', 'came', 'after', 'White', 'House', 'Press', 'Secretary', 'Sean', 'Spicer', 'cited', 'claims', 'first', 'made', 'on', 'US', 'TV', 'channel', 'Fox', 'News', 'earlier', 'this', 'week', '.', 'GCHQ', 'responded', 'that', 'the', 'allegations', 'were', '“', 'nonsense', ',', 'utterly', 'ridiculous', 'and', 'should', 'be', 'ignored', '”', '.', 'Former', 'judge', 'Andrew', 'Napolitano', 'initially', 'made', 'the', 'claims', 'of', 'GCHQ', 'involvement', '.', 'Mr', 'Spicer', 'quoted', 'Mr', 'Napolitano', ':', '“', 'Three', 'intelligence', 'sources', 'have', 'informed', 'Fox', 'News', 'that', 'President', 'Obama', 'went', 'outside', 'the', 'chain', 'of', 'command', '.', '”']\n",
      "\n",
      "Lemmatized tokens: ['Britain', '’', 's', 'communication', 'intelligence', 'agency', 'GCHQ', 'ha', 'issued', 'a', 'statement', 'denying', 'it', 'wiretapped', 'Donald', 'Trump', 'during', 'the', 'US', 'presidential', 'campaign', '.', 'The', 'unusual', 'move', 'by', 'the', 'agency', 'came', 'after', 'White', 'House', 'Press', 'Secretary', 'Sean', 'Spicer', 'cited', 'claim', 'first', 'made', 'on', 'US', 'TV', 'channel', 'Fox', 'News', 'earlier', 'this', 'week', '.', 'GCHQ', 'responded', 'that', 'the', 'allegation', 'were', '“', 'nonsense', ',', 'utterly', 'ridiculous', 'and', 'should', 'be', 'ignored', '”', '.', 'Former', 'judge', 'Andrew', 'Napolitano', 'initially', 'made', 'the', 'claim', 'of', 'GCHQ', 'involvement', '.', 'Mr', 'Spicer', 'quoted', 'Mr', 'Napolitano', ':', '“', 'Three', 'intelligence', 'source', 'have', 'informed', 'Fox', 'News', 'that', 'President', 'Obama', 'went', 'outside', 'the', 'chain', 'of', 'command', '.', '”']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatisation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "print(\"Original tokens:\", tokens)\n",
    "print()\n",
    "print(\"Lemmatized tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "346423d2-1ef2-41c6-9197-546b673b1f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['Britain', '’', 's', 'communications', 'intelligence', 'agency', 'GCHQ', 'has', 'issued', 'a', 'statement', 'denying', 'it', 'wiretapped', 'Donald', 'Trump', 'during', 'the', 'US', 'presidential', 'campaign', '.', 'The', 'unusual', 'move', 'by', 'the', 'agency', 'came', 'after', 'White', 'House', 'Press', 'Secretary', 'Sean', 'Spicer', 'cited', 'claims', 'first', 'made', 'on', 'US', 'TV', 'channel', 'Fox', 'News', 'earlier', 'this', 'week', '.', 'GCHQ', 'responded', 'that', 'the', 'allegations', 'were', '“', 'nonsense', ',', 'utterly', 'ridiculous', 'and', 'should', 'be', 'ignored', '”', '.', 'Former', 'judge', 'Andrew', 'Napolitano', 'initially', 'made', 'the', 'claims', 'of', 'GCHQ', 'involvement', '.', 'Mr', 'Spicer', 'quoted', 'Mr', 'Napolitano', ':', '“', 'Three', 'intelligence', 'sources', 'have', 'informed', 'Fox', 'News', 'that', 'President', 'Obama', 'went', 'outside', 'the', 'chain', 'of', 'command', '.', '”']\n",
      "\n",
      "Stemmed tokens: ['britain', '’', 's', 'commun', 'intellig', 'agenc', 'gchq', 'ha', 'issu', 'a', 'statement', 'deni', 'it', 'wiretap', 'donald', 'trump', 'dure', 'the', 'us', 'presidenti', 'campaign', '.', 'the', 'unusu', 'move', 'by', 'the', 'agenc', 'came', 'after', 'white', 'hous', 'press', 'secretari', 'sean', 'spicer', 'cite', 'claim', 'first', 'made', 'on', 'us', 'tv', 'channel', 'fox', 'new', 'earlier', 'thi', 'week', '.', 'gchq', 'respond', 'that', 'the', 'alleg', 'were', '“', 'nonsens', ',', 'utterli', 'ridicul', 'and', 'should', 'be', 'ignor', '”', '.', 'former', 'judg', 'andrew', 'napolitano', 'initi', 'made', 'the', 'claim', 'of', 'gchq', 'involv', '.', 'mr', 'spicer', 'quot', 'mr', 'napolitano', ':', '“', 'three', 'intellig', 'sourc', 'have', 'inform', 'fox', 'new', 'that', 'presid', 'obama', 'went', 'outsid', 'the', 'chain', 'of', 'command', '.', '”']\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "print(\"Original tokens:\", tokens)\n",
    "print()\n",
    "print(\"Stemmed tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3378622d-327d-489c-abdd-4c06ddcd1ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Britain, Label: GPE\n",
      "Entity: GCHQ, Label: ORGANIZATION\n",
      "Entity: Donald Trump, Label: PERSON\n",
      "Entity: US, Label: ORGANIZATION\n",
      "Entity: White House, Label: FACILITY\n",
      "Entity: Sean Spicer, Label: PERSON\n",
      "Entity: US, Label: ORGANIZATION\n",
      "Entity: Fox News, Label: PERSON\n",
      "Entity: GCHQ, Label: ORGANIZATION\n",
      "Entity: Andrew Napolitano, Label: PERSON\n",
      "Entity: GCHQ, Label: ORGANIZATION\n",
      "Entity: Spicer, Label: PERSON\n",
      "Entity: Fox News, Label: PERSON\n",
      "Entity: Obama, Label: PERSON\n"
     ]
    }
   ],
   "source": [
    "#named entity recognition\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "tags = pos_tag(tokens)\n",
    "ner_tags = ne_chunk(tags)\n",
    "\n",
    "for entity in ner_tags:\n",
    "    if isinstance(entity, nltk.Tree):\n",
    "        entity_words = [word for word, tag in entity.leaves()]\n",
    "        entity_name = \" \".join(entity_words)\n",
    "        entity_label = entity.label()\n",
    "        print(f\"Entity: {entity_name}, Label: {entity_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5e2ee83-4bac-4b64-b34f-a24af0f9f842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter the sentence: I shot an elephant in my pajamas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "     S                                       \n",
      "  ___|______________                          \n",
      " |                  VP                       \n",
      " |         _________|__________               \n",
      " |        VP                   PP            \n",
      " |    ____|___              ___|___           \n",
      " |   |        NP           |       NP        \n",
      " |   |     ___|_____       |    ___|_____     \n",
      " NP  V   Det        N      P  Det        N   \n",
      " |   |    |         |      |   |         |    \n",
      " I  shot  an     elephant  in  my     pajamas\n",
      "\n",
      "\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n",
      "     S                                   \n",
      "  ___|__________                          \n",
      " |              VP                       \n",
      " |    __________|______                   \n",
      " |   |                 NP                \n",
      " |   |     ____________|___               \n",
      " |   |    |     |          PP            \n",
      " |   |    |     |       ___|___           \n",
      " |   |    |     |      |       NP        \n",
      " |   |    |     |      |    ___|_____     \n",
      " NP  V   Det    N      P  Det        N   \n",
      " |   |    |     |      |   |         |    \n",
      " I  shot  an elephant  in  my     pajamas\n",
      "\n",
      "\n",
      "the given sentence is ambiguous\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import PCFG\n",
    "from nltk import parse,ChartParser\n",
    "\n",
    "#finding the ambiguity of the sentence using parse tree\n",
    "grammar1=nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "count=0\n",
    "sentence=input(\"enter the sentence:\")\n",
    "sent=sentence.split()\n",
    "rd_parse=nltk.ChartParser(grammar1)\n",
    "for tree in rd_parse.parse(sent):\n",
    "    print(tree)\n",
    "    tree.pretty_print()\n",
    "    print()\n",
    "    count=count+1\n",
    "if(count>1):\n",
    "    print(\"the given sentence is ambiguous\")\n",
    "else:\n",
    "    print(\"the given sentence is unambiguous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6506341-8d82-4f86-8ccb-31fbd23ff8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
